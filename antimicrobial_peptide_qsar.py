# -*- coding: utf-8 -*-
"""Antimicrobial_Peptide_QSAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L92W0jKnhDGMaxf26gsdlSFe1Ov_dcP4

# **Predicting Activity of Short Antimicrobial Peptides through Physico-chemical Properties of the Whole Protein**

Original notebook by [Chanin Nantasenamat](https://medium.com/@chanin.nantasenamat) aka [Data Professor](http://youtube.com/dataprofessor)

Dataset:
[Paper](https://www.cell.com/molecular-therapy-family/nucleic-acids/fulltext/S2162-2531(20)30132-3) | [Dataset](https://cbbio.online/AxPEP/?action=dataset)

# **Install conda**
"""

################################################################################
# INSTALL CONDA ON GOOGLE COLAB
################################################################################
! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

"""# **Download and Install Pfeature**"""

! wget https://github.com/raghavagps/Pfeature/raw/master/PyLib/Pfeature.zip

! unzip Pfeature.zip

import os
os.chdir('Pfeature')

#! python setup.py install

"""# **Install CD-HIT**"""

! conda install -c bioconda cd-hit -y

"""# **Load peptide dataset**"""

! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_po.fasta

! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_ne.fasta

! cat train_ne.fasta

"""# **Remove redundant sequences using CD-HIT**"""

! cd-hit -i train_po.fasta -o train_po_cdhit.txt -c 0.99

! cd-hit -i train_ne.fasta -o train_ne_cdhit.txt -c 0.99

! ls -l

! grep ">" train_po_cdhit.txt | wc -l

! grep ">" train_po.fasta | wc -l

! grep ">" train_ne.fasta | wc -l

! grep ">" train_ne_cdhit.txt | wc -l

"""# **Calculate features using the Pfeature library**

Feature classes provided by Pfeature is summarized in the tables below.

**Composition Based Features**

Feature claass | Description | Function
---|---|---
AAC | Amino acid composition | aac_wp
DPC | Dipeptide composition | dpc_wp
TPC | Tripeptide composition | tpc_wp
ABC | Atom and bond composition | atc_wp, btc_wp
PCP | Physico-chemical properties | pcp_wp
AAI | Amino acid index composition | aai_wp
RRI | Repetitive Residue Information | rri_wp
DDR | Distance distribution of residues |ddr_wp
PRI | Physico-chemical properties repeat composition | pri_wp
SEP | Shannon entropy | sep_wp
SER | Shannon entropy of residue level | ser_wp
SPC | Shannon entropy of physicochemical property | spc_wp
ACR | Autocorrelation | acr_wp
CTC | Conjoint Triad Calculation | ctc_wp
CTD | Composition enhanced transition distribution | ctd_wp
PAAC | Pseudo amino acid composition | paac_wp
APAAC | Amphiphilic pseudo amino acid composition | apaac_wp
QSO | Quasi sequence order | qos_wp
SOC | Sequence order coupling | soc_wp

[Pfeature Manual](https://webs.iiitd.edu.in/raghava/pfeature/Pfeature_Manual.pdf)

### **Define functions for calculating the different features**
"""

import pandas as pd

#  pcp_wp

from Pfeature.pfeature import pcp_wp

def pcp(input):
  a = input.rstrip('txt')
  output = a + 'pcp.csv'
  df_out = pcp_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

pcp('train_po_cdhit.txt')

# Amino acid composition (AAC)

from Pfeature.pfeature import aac_wp

def aac(input):
  a = input.rstrip('txt')
  output = a + 'aac.csv'
  df_out = aac_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

aac('train_po_cdhit.txt')

# Dipeptide composition (DPC)

from Pfeature.pfeature import dpc_wp

def dpc(input):
  a = input.rstrip('txt')
  output = a + 'dpc.csv'
  df_out = dpc_wp(input, output, 1)
  df_in = pd.read_csv(output)
  return df_in

feature = dpc('train_po_cdhit.txt')
feature

"""### **Calculate feature for both positive and negative classes + combines the two classes + merge with class labels**"""

pos = 'train_po_cdhit.txt'
neg = 'train_ne_cdhit.txt'

def feature_calc(po, ne, feature_name):
  # Calculate feature
  po_feature = feature_name(po)
  ne_feature = feature_name(ne)
  # Create class labels
  po_class = pd.Series(['positive' for i in range(len(po_feature))])
  ne_class = pd.Series(['negative' for i in range(len(ne_feature))])
  # Combine po and ne
  po_ne_class = pd.concat([po_class, ne_class], axis=0)
  po_ne_class.name = 'class'
  po_ne_feature = pd.concat([po_feature, ne_feature], axis=0)
  # Combine feature and class
  df = pd.concat([po_ne_feature, po_ne_class], axis=1)
  return df

#feature = feature_calc(pos, neg, aac) # AAC
#feature = feature_calc(pos, neg, dpc) # DPC
feature = feature_calc(pos, neg, pcp) # PCP
feature

"""# **Data pre-processing**"""

feature

# Assigns the features to X and class label to Y
X = feature.drop('class', axis=1)
y = feature['class'].copy()

# Encoding the Y class label
y = y.map({"positive": 1, "negative": 0})

X.shape

!pip install scikit-learn

# Feature selection (Variance threshold)
from sklearn.feature_selection import VarianceThreshold

fs = VarianceThreshold(threshold=0.1)
fs.fit_transform(X)
#X2.shape
X2 = X.loc[:, fs.get_support()]
X2

# Data split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state =42, stratify=y)

"""---

# **Random Forest**
"""

# Build random forest model

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500)

rf.fit(X_train, y_train)

"""### **Apply the model to make predictions**"""

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

"""### **Model performance**"""

feature['class']

# Simplest and quickest way to obtain the model performance (Accuracy)
rf.score(X_test,y_test)

# Accuracy
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_test_pred)

# Matthew Correlation Coefficient
from sklearn.metrics import matthews_corrcoef

matthews_corrcoef(y_test, y_test_pred)

# Confusion matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_test_pred)

# Classification report
from sklearn.metrics import classification_report

model_report = classification_report(y_train, y_train_pred, target_names=['positive','negative'])

f = open('model_report.txt','w')
f.writelines(model_report)
f.close()

!pip install matplotlib

# ROC curve
import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay

# Assuming 'rf' is your trained model (e.g., RandomForest)
RocCurveDisplay.from_estimator(rf, X_test, y_test)

plt.show()

RocCurveDisplay.from_estimator(rf, X_test, y_test)
plt.show()

"""### **Feature importance**"""

# Display Dataframe of the dataset after feature selection (variance threshold)
X2

# Retrieve feature importance from the RF model
importance = pd.Series(rf.feature_importances_, name = 'Gini')

# Retrieve feature names
feature_names = pd.Series(X2.columns, name = 'Feature')

# Combine feature names and Gini values into a Dataframe
df = pd.concat([feature_names, importance], axis=1, names=['Feature', 'Gini'])
df

# Plot of feature importance
import matplotlib.pyplot as plt
import seaborn as sns

df_sorted = df.sort_values('Gini', ascending=False)[:20] # Sort by Gini in descending order; Showing only the top 20 results

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(x = 'Gini', y = 'Feature', data = df_sorted)
plt.xlabel("Feature Importance")

"""---"""